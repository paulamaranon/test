import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
import csv
from datetime import datetime, timedelta

# Set random seeds for reproducibility
np.random.seed(42)
tf.random.set_seed(42)

# Read data from CSV file
#df = pd.read_csv(r"claims.txt", parse_dates=["Date"], index_col=["Date"], encoding="utf-16")
# Extract tickets and time_step
#tickets = df["Number"].to_numpy()
#time_step = np.arange(len(tickets))

import csv
from datetime import datetime

time_step = []
tickets = []

with open("claims.txt", "r", encoding="utf-16") as f:
        csv_reader = csv.reader(f, delimiter=",")

        # Skip the header
        next(csv_reader)

        # Skip lines with NUL characters
        lines = (line for line in csv_reader if "\0" not in line)

        # Iterate through non-null lines
        for line in lines:
            # Assuming the first column is the date and the second column is the number
            time_step.append(datetime.strptime(line[0], "%Y-%m-%d"))
            tickets.append(float(line[1]))

# View the first 10 of each
print(time_step[:10], tickets[:10])
print(len(time_step))

split_time = 18

time_train = time_step[:split_time]
x_train = tickets[:split_time]

time_valid = time_step[split_time:]
x_valid = tickets[split_time:]

# Use Min-Max scaling
#SCALER needs to be used on numpy array
scaler = MinMaxScaler()
x_train_scaled = scaler.fit_transform(np.array(x_train).reshape(-1, 1)).flatten()
x_valid_scaled = scaler.transform(np.array(x_valid).reshape(-1, 1)).flatten()


# Function for creating a windowed dataset
def windowed_dataset(series, window_size):
    dataset = tf.data.Dataset.from_tensor_slices(series)
    dataset = dataset.window(window_size, shift=1, drop_remainder=True)
    dataset = dataset.flat_map(lambda window: window.batch(window_size))
    dataset = dataset.map(lambda window: (window[:-1], window[-1]))
    return dataset.batch(1).prefetch(1)

# Set window size
window_size =3   # Choose a larger window size

# Create windowed dataset for training
dataset_train = windowed_dataset(x_train_scaled, window_size)
# Create windowed dataset for validation
dataset_valid = windowed_dataset(x_valid_scaled, window_size)

# Build a more complex LSTM model


# SIMPLE RNN
model = tf.keras.models.Sequential([
    tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1), input_shape=[window_size]),
    tf.keras.layers.SimpleRNN(48, return_sequences=True),
    tf.keras.layers.SimpleRNN(24),
    tf.keras.layers.Dense(1)
])
'''

# dense
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(65, input_shape=[window_size], activation="relu"),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(1)
]) 
model = tf.keras.models.Sequential([
    tf.keras.layers.LSTM(72, return_sequences=True, input_shape=[None, 1]),
    tf.keras.layers.LSTM(256, return_sequences=True),
    tf.keras.layers.LSTM(256),
    tf.keras.layers.Dropout(0.1),  # Add dropout layer
    tf.keras.layers.Dense(1)
])


model = tf.keras.models.Sequential([
  tf.keras.layers.Conv1D(filters=64, kernel_size=3,
                      strides=1,
                      activation="relu",
                      padding='causal',
                      input_shape=[window_size, 1]),
  tf.keras.layers.LSTM(64, return_sequences=True),
  tf.keras.layers.LSTM(64),
  tf.keras.layers.Dense(30, activation="relu"),
  tf.keras.layers.Dense(10, activation="relu"),
  tf.keras.layers.Dense(1)
  #tf.keras.layers.Lambda(lambda x: x * 100)
])

# Build the Model
model = tf.keras.models.Sequential([
  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),
                     input_shape=[window_size]),
  tf.keras.layers.SimpleRNN(40, return_sequences=True),
  tf.keras.layers.SimpleRNN(40),
  tf.keras.layers.Dense(1)
  #tf.keras.layers.Lambda(lambda x: x * 100.0)
])
'''

'''
###TUNE THE LEARNING RATE
# Set the learning rate scheduler
lr_schedule = tf.keras.callbacks.LearningRateScheduler(
    lambda epoch: 1e-8 * 10**(int(epoch) / 20), verbose=1)
# Initialize the optimizer
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-8)
# Set the training parameters
model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])
# Train the model
history = model.fit(dataset_train, epochs=3, callbacks=[lr_schedule])

# Define the learning rate array
lrs = 1e-8 * (10 ** (np.arange(3) / 20))

# Set the figure size
plt.figure(figsize=(10, 6))
# Set the grid
plt.grid(True)
# Plot the loss in log scale
plt.semilogx(lrs, history.history["loss"])
# Increase the tickmarks size
plt.tick_params('both', length=10, width=1, which='both')
# Set the plot boundaries
plt.axis([1e-8, 1e-1, 0, 1])

#########TRAIN THE MODEL WITH THE PROPER LEARNING RATE
# Reset states generated by Keras
tf.keras.backend.clear_session()

'''
# Compile the model
#model.compile(loss="mse", optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3))
# Set the learning rate
learning_rate = 8e-4
# Set the optimizer
#optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate)
model.compile(loss=tf.keras.losses.Huber(), optimizer=optimizer, metrics=['mae'])

#early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
# Train the model
history = model.fit(dataset_train, epochs=30, validation_data=dataset_valid)#, callbacks=[early_stopping])

# Plot the original data, training data, test data, and predictions
plt.figure(figsize=(12, 6))

# Plot original data
plt.subplot(2, 1, 1)
plt.plot(time_step, tickets, label='Original Data', marker='o', linestyle='-', color='black')

# Plot training data (inverse transform x_train_scaled)
plt.plot(time_step[:split_time], scaler.inverse_transform(x_train_scaled.reshape(-1, 1)).flatten(), label='Training Data', color='blue')

# Plot validation data (inverse transform x_valid_scaled)
plt.plot(time_step[split_time:], scaler.inverse_transform(x_valid_scaled.reshape(-1, 1)).flatten(), label='Validation Data', color='red')

# Make predictions on validation set for the entire duration

# Make predictions on validation set for the entire duration
forecast_valid_scaled = model.predict(dataset_valid)

# Flatten the predictions and inverse transform
forecast_valid = scaler.inverse_transform(forecast_valid_scaled.flatten().reshape(-1, 1)).flatten()

# Use the same indexing as for validation data
time_forecast_valid = time_step[split_time:split_time + len(forecast_valid)]

# Ensure dimensions match
assert len(time_forecast_valid) == len(forecast_valid), "Dimensions do not match"

# Extend predictions beyond the validation set############################################
# Initialize an array to store predictions
forecast_future_scaled = []

# Use the last window_size steps of the validation set to initialize predictions
current_window = np.array([x_valid_scaled[-window_size:]])

# Generate predictions iteratively
for i in range(len(time_valid) + 1):
    # Predict one time step ahead
    forecast_future_scaled.append(model.predict(current_window.reshape(1, -1, 1))[0, 0])

    # Update the current window for the next prediction
    current_window = np.append(current_window[1:], forecast_future_scaled[-1])

# Inverse transform the scaled predictions
forecast_future = scaler.inverse_transform(np.array(forecast_future_scaled).reshape(-1, 1)).flatten()

# Update the time and forecast arrays
time_forecast_valid = np.append(time_forecast_valid,
                                time_forecast_valid[-1] + pd.to_timedelta(np.arange(1, len(forecast_future) + 1),
                                                                          unit='D'))
time_forecast_valid = pd.to_datetime(time_forecast_valid)
forecast_valid = np.append(forecast_valid, forecast_future)

# Update the time_step array to include the extended time steps for the forecast
# Update the time_step array to include the extended time steps for the forecast
time_step_extended = np.append(time_step, time_forecast_valid)

# Plot predictions on validation set along the extended time steps
plt.plot(time_step_extended, tickets[:len(time_step_extended)], label='Original Data', marker='o', linestyle='-', color='black')
plt.plot(time_step[:split_time], scaler.inverse_transform(x_train_scaled.reshape(-1, 1)).flatten(), label='Training Data', color='blue')
plt.plot(time_step[split_time:], scaler.inverse_transform(x_valid_scaled.reshape(-1, 1)).flatten(), label='Validation Data', color='red')
plt.plot(time_forecast_valid, forecast_valid, label='Predictions', linestyle='--', color='green')






plt.title('Original Data, Training Data, Validation Data, and Predictions')
plt.xlabel('Time Step')
plt.ylabel('Values')
plt.legend()
plt.grid(True)

# Plot validation loss vs training loss
plt.subplot(2, 1, 2)
plt.plot(history.history['loss'], label='Training Loss', color='blue')
plt.plot(history.history['val_loss'], label='Validation Loss', color='red')
plt.plot(history.history['mae'], label='Training MAE', color='green')
plt.plot(history.history['val_mae'], label='Validation MAE', color='orange')

plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
